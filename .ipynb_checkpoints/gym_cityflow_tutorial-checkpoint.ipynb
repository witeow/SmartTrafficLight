{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67277642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e919f",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a1be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym_cityflow\n",
    "# from gym_cityflow.envs import Cityflow\n",
    "import gym\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import cityflow\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from gym import spaces\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6385cbb",
   "metadata": {},
   "source": [
    "## Create gym environment for Cityflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dddd2f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naction space = num of lighPhases\\n\\n\\n                |\\n                |\\n                N\\n                |\\n                |\\n------W----[][][][][]-------E-------\\n                |\\n                |\\n                S\\n                |\\n                |\\n\\n\\ngo straight (W/E), (N/S)\\nstraight + right (N), (S), (E), (W)\\nright only (W/E), (N/S)\\n\\ntotal 8 phases\\n################################################################\\n\\nobservation space = [for each intersection[for each phase [num of incoming vehicles for each phase, avg speed for that phase]]]\\n\\n################################################################\\n\\nneeds to be a way to extract number of lightPhases for each intersection (done, roadnet file)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "action space = num of lighPhases\n",
    "\n",
    "\n",
    "                |\n",
    "                |\n",
    "                N\n",
    "                |\n",
    "                |\n",
    "------W----[][][][][]-------E-------\n",
    "                |\n",
    "                |\n",
    "                S\n",
    "                |\n",
    "                |\n",
    "\n",
    "\n",
    "go straight (W/E), (N/S)\n",
    "straight + right (N), (S), (E), (W)\n",
    "right only (W/E), (N/S)\n",
    "\n",
    "total 8 phases\n",
    "################################################################\n",
    "\n",
    "observation space = [for each intersection[for each phase [num of incoming vehicles for each phase, avg speed for that phase]]]\n",
    "\n",
    "################################################################\n",
    "\n",
    "needs to be a way to extract number of lightPhases for each intersection (done, roadnet file)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb4895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cityFlowGym(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, config_path, episodeSteps):\n",
    "        \n",
    "        # edit config_path to change replayLogFile name for each step\n",
    "#         with open(config_path, \"r+\") as jsonFile:\n",
    "#             data = json.load(jsonFile)\n",
    "#             fileName = data[\"replayLogFile\"].split(\".\")[0]\n",
    "#             data[\"replayLogFile\"] = fileName + \"_\" + str(replayNumber) + \".txt\"\n",
    "\n",
    "#             jsonFile.seek(0)  # rewind\n",
    "#             json.dump(data, jsonFile)\n",
    "        \n",
    "        # creating Cityflow engine\n",
    "        self.engine = cityflow.Engine(config_path, thread_num=1)\n",
    "        \n",
    "        #open cityflow config file into dict\n",
    "        self.configDict = json.load(open(config_path))\n",
    "        #open cityflow roadnet file into dict\n",
    "        self.roadnetDict = json.load(open(self.configDict['dir'] + self.configDict['roadnetFile']))\n",
    "        self.flowDict = json.load(open(self.configDict['dir'] + self.configDict['flowFile']))\n",
    "        \n",
    "        #steps per episode\n",
    "#         self.steps_per_episode = episodeSteps\n",
    "        self.isDone = False\n",
    "        self.currStep = 0\n",
    "        self.maxStep = episodeSteps\n",
    "        self.minLighphaseTime = 0 #approximate realistic time\n",
    "        self.maxLighphaseTime = 60 #approximate realistic time\n",
    "        self.maxSpeed = self.flowDict[0][\"vehicle\"][\"maxSpeed\"]\n",
    "        self.info = {}\n",
    "        self.maxLightPhase = 0\n",
    "        \n",
    "        # create dict of controllable intersections and number of light phases\n",
    "        self.intersections = {}\n",
    "        for i in range(len(self.roadnetDict['intersections'])):\n",
    "            # check if intersection is controllable\n",
    "            if self.roadnetDict['intersections'][i]['virtual'] == False:\n",
    "                currIntersection = self.roadnetDict['intersections'][i]\n",
    "                \n",
    "                # change maxLightPhase if needed for observationSpace definition\n",
    "                if self.maxLightPhase < len(currIntersection['trafficLight']['lightphases']):\n",
    "                    self.maxLightPhase = len(currIntersection['trafficLight']['lightphases'])\n",
    "                \n",
    "                # create incoming roadlink key that contains all incoming road lane id for each roadLink index\n",
    "                roadLinkIntersectionDict = {}\n",
    "                for roadLinkIndex, roadLink in enumerate(currIntersection[\"roadLinks\"]):\n",
    "                    startRoad = currIntersection[\"roadLinks\"][roadLinkIndex][\"startRoad\"]\n",
    "                    roadLinkSet = set()\n",
    "                    for laneLinkIndex, laneLink in enumerate(currIntersection[\"roadLinks\"][roadLinkIndex][\"laneLinks\"]):\n",
    "                        tempLaneLink = startRoad + \"_\" + str(laneLink[\"startLaneIndex\"])\n",
    "                        roadLinkSet.add(tempLaneLink)\n",
    "                    roadLinkIntersectionDict[roadLinkIndex] = roadLinkSet\n",
    "                    \n",
    "                # using the roadLinkIntersectionDict, we create the incoming road lanes and store it to each lightphase    \n",
    "                lightPhaseRoadLaneIntersectionDict = {}\n",
    "                for lightPhase in range(len(currIntersection['trafficLight']['lightphases'])):\n",
    "                    availableRoadLinks = set()\n",
    "                    for roadLinkIndex in currIntersection['trafficLight']['lightphases'][lightPhase][\"availableRoadLinks\"]:\n",
    "                        availableRoadLinks.update(roadLinkIntersectionDict[roadLinkIndex])\n",
    "                    lightPhaseRoadLaneIntersectionDict[lightPhase] = availableRoadLinks\n",
    "\n",
    "                # add intersection to dict where key = intersection_id\n",
    "                # value = no of lightPhases, incoming lane names, outgoing lane names, directions for each lane group\n",
    "                self.intersections[self.roadnetDict['intersections'][i]['id']] = { \n",
    "                    \"lightPhases\" : lightPhaseRoadLaneIntersectionDict,\n",
    "                    \"incomingRoadLinks\" : roadLinkIntersectionDict\n",
    "                }\n",
    "        \n",
    "        #setup intersectionNames list for agent actions\n",
    "        self.intersectionNames = []\n",
    "        for key in self.intersections:\n",
    "            self.intersectionNames.append(key)\n",
    "        \n",
    "        #define action space (num of lightPhases for each intersection) MultiDiscrete()\n",
    "        actionSpaceArray = []\n",
    "        upperBound = []\n",
    "        for intersection in self.intersections:\n",
    "            upperBound.append(len(self.intersections[intersection][\"lightPhases\"]))\n",
    "            upperBound.append(self.maxLighphaseTime)\n",
    "#             lightphaseDurationSpace = self.minLighphaseTime = 0 #approximate realistic time\n",
    "        \n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            np.array([0 for space in range(len(upperBound))]).astype(np.int32),\n",
    "            np.array(upperBound).astype(np.int32),\n",
    "            dtype = np.int32\n",
    "        )\n",
    "        \n",
    "        # define observation space\n",
    "        numOfIntersections = len(self.intersectionNames)\n",
    "        observationSpace = {\n",
    "                \"numVehicles\" : spaces.Box(0, np.inf, (numOfIntersections, self.maxLightPhase), dtype=np.int32),\n",
    "                \"numWaitingVehicles\" : spaces.Box(0, np.inf, (numOfIntersections, self.maxLightPhase), dtype=np.int32),\n",
    "                \"avgSpeed\" : spaces.Box(0, np.inf, (numOfIntersections, self.maxLightPhase), dtype=np.float32)\n",
    "\n",
    "            }\n",
    "        \n",
    "        self.observation_space = spaces.Dict(observationSpace)\n",
    "\n",
    "    def step(self, action):\n",
    "        actionArr = np.ndarray.tolist(action)\n",
    "        minTimer = -1\n",
    "        negRewards = False # true means set negative infinity if lightphase action given is not part of the maxlightphases\n",
    "        for intersection in range(len(self.intersectionNames)):\n",
    "            # check if lightphase in intersection\n",
    "            if actionArr[intersection*2] <= len(self.intersections[self.intersectionNames[intersection]][\"lightPhases\"]):\n",
    "                self.engine.set_tl_phase(self.intersectionNames[intersection], int(actionArr[intersection*2]))\n",
    "                if actionArr[1+intersection*2] < minTimer:\n",
    "                    minTimer = intersection[1]\n",
    "                    \n",
    "            else:\n",
    "                negRewards = True\n",
    "                \n",
    "        # let scenario run for minTimer long\n",
    "        if minTimer == -1:\n",
    "            minTimer = 10\n",
    "        \n",
    "        for second in range(minTimer):\n",
    "            self.engine.next_step()\n",
    "        \n",
    "        obs = self.get_observation()\n",
    "        reward = self.get_reward(obs, negRewards)\n",
    "        self.isDone = self.currStep >= self.maxStep\n",
    "        info = {}\n",
    "        self.currStep += 1\n",
    "        return obs, reward, self.isDone, info\n",
    "    \n",
    "    def get_observation(self):\n",
    "        # get waiting vehicles for each lane first (key = laneID, value = numVehiclesWaiting)\n",
    "        vehiclesWaitingByLaneDict = self.engine.get_lane_waiting_vehicle_count()\n",
    "#         print(vehiclesWaitingByLaneDict)\n",
    "        \n",
    "        # get all vehicles speed (key = vehId, value = speed)\n",
    "        vehiclesSpeedDict = self.engine.get_vehicle_speed()\n",
    "#         print(vehiclesSpeedDict)\n",
    "        \n",
    "        # get all vehicles for each lane (key = laneId, value = [vehId])\n",
    "        vehicleLaneDict = self.engine.get_lane_vehicles()\n",
    "#         print(vehicleLaneDict)\n",
    "        \n",
    "        # create observation space for number of waiting vehicles and avgSpeed of moving+waiting vehicles of each lane\n",
    "        # , for each lightphase\n",
    "        \n",
    "        # create for each roadlane first\n",
    "        '''\n",
    "        for roadLaneDict = {intersectionId : \n",
    "                                {roadLaneId: \n",
    "                                    {\"numVehicles\" : int,\n",
    "                                     \"numWaitingVehicles\" : int,\n",
    "                                     \"avgSpeed\" : float\n",
    "                                                \n",
    "                                    }\n",
    "                \n",
    "                                }\n",
    "        \n",
    "                            }\n",
    "        '''\n",
    "        roadLaneDict = {}\n",
    "        for intersectionId, intersectionValue in self.intersections.items():\n",
    "            roadLaneByIntersectionDict = {}\n",
    "            \n",
    "            # get a set of all incoming roadlanes for the intersection\n",
    "            roadLaneByIntersectionSet = set()\n",
    "            for roadLane in intersectionValue[\"incomingRoadLinks\"].values():\n",
    "                roadLaneByIntersectionSet.update(roadLane)\n",
    "            \n",
    "            # for each roadlane, find out num of waiting vehicles, and vehicles (waiting + nonwaiting) with speed\n",
    "            for roadLane in roadLaneByIntersectionSet:\n",
    "                tempRoadLaneDict = {}\n",
    "                tempVehiclesArr = vehicleLaneDict[roadLane]\n",
    "                tempRoadLaneDict[\"numVehicles\"] = len(tempVehiclesArr)\n",
    "                tempRoadLaneDict[\"numWaitingVehicles\"] = vehiclesWaitingByLaneDict[roadLane]\n",
    "                if len(tempVehiclesArr) == 0:\n",
    "                    tempRoadLaneDict[\"avgSpeed\"] = 0\n",
    "                else:\n",
    "                    tempAvgSpeed = 0\n",
    "                    for vehicle in tempVehiclesArr:\n",
    "                        tempAvgSpeed += vehiclesSpeedDict[vehicle]\n",
    "                    tempRoadLaneDict[\"avgSpeed\"] = tempAvgSpeed/len(tempVehiclesArr)\n",
    "                \n",
    "                roadLaneByIntersectionDict[roadLane] = tempRoadLaneDict\n",
    "                \n",
    "            # add intersection to roadLaneDict\n",
    "            roadLaneDict[intersectionId] = roadLaneByIntersectionDict\n",
    "        \n",
    "        \n",
    "        \n",
    "        # define observation space\n",
    "        observationSpace = {\n",
    "                \"numVehicles\" : [],\n",
    "                \"numWaitingVehicles\" : [],             \n",
    "                \"avgSpeed\" : []\n",
    "            }\n",
    "        \n",
    "        # for each intersection \n",
    "        for intersectionId, intersectionValue in self.intersections.items():\n",
    "            numVehiclesArr = []\n",
    "            numWaitingVehiclesArr = []             \n",
    "            avgSpeedArr = []\n",
    "            \n",
    "            # for each lightphase in the intersection\n",
    "            for lightPhase, roadLaneArr in intersectionValue[\"lightPhases\"].items():\n",
    "                totalVehiclesByPhase = 0\n",
    "                waitingVehiclesByPhase = 0\n",
    "                totalSpeedByPhase = 0\n",
    "                \n",
    "                # for each roadLane (availableRoadLinks) in each lightphase\n",
    "                for roadLaneId in roadLaneArr:\n",
    "                    totalVehiclesByPhase += roadLaneDict[intersectionId][roadLaneId][\"numVehicles\"]\n",
    "                    waitingVehiclesByPhase += roadLaneDict[intersectionId][roadLaneId][\"numWaitingVehicles\"]\n",
    "                    totalSpeedByPhase += (roadLaneDict[intersectionId][roadLaneId][\"numVehicles\"] * \n",
    "                                          roadLaneDict[intersectionId][roadLaneId][\"avgSpeed\"])\n",
    "                    \n",
    "                # error checking if theres no vehicles in the lane to eliminate divide by zero error\n",
    "                if totalVehiclesByPhase == 0:\n",
    "                    avgSpeedByPhase = 0\n",
    "                else:\n",
    "                    avgSpeedByPhase = totalSpeedByPhase/totalVehiclesByPhase\n",
    "                    \n",
    "                numVehiclesArr.append(totalVehiclesByPhase)\n",
    "                numWaitingVehiclesArr.append(waitingVehiclesByPhase)\n",
    "                avgSpeedArr.append(avgSpeedByPhase)\n",
    "                \n",
    "            # convert to np array\n",
    "            tempNumVehicles = np.array(numVehiclesArr)\n",
    "            tempNumWaitingVehicles = np.array(numWaitingVehiclesArr)\n",
    "            tempAvgSpeed = np.array(avgSpeedArr)\n",
    "            \n",
    "            observationSpace[\"numVehicles\"].append(tempNumVehicles)\n",
    "            observationSpace[\"numWaitingVehicles\"].append(tempNumWaitingVehicles)\n",
    "            observationSpace[\"avgSpeed\"].append(tempAvgSpeed)\n",
    "                \n",
    "        # convert to np array\n",
    "        observationSpace[\"numVehicles\"] = np.array(observationSpace[\"numVehicles\"])\n",
    "        observationSpace[\"numWaitingVehicles\"] = np.array(observationSpace[\"numWaitingVehicles\"])\n",
    "        observationSpace[\"avgSpeed\"] = np.array(observationSpace[\"avgSpeed\"])\n",
    "\n",
    "        return observationSpace\n",
    "        \n",
    "    def get_reward(self, observationSpace, negRewards):\n",
    "        \n",
    "        # if model returns lightphase that is not possible\n",
    "        if negRewards == True:\n",
    "            return -np.inf\n",
    "        # aggregate total reward from all intersections\n",
    "        totalReward = 0\n",
    "        \n",
    "        # numVehicles reward calc\n",
    "        for intersection in observationSpace[\"numVehicles\"]:\n",
    "            for veh in intersection:\n",
    "                \n",
    "                totalReward -= veh\n",
    "#                 print(\"veh\", totalReward)\n",
    "                \n",
    "        # numWaitingVehicles reward calc\n",
    "        for intersection in observationSpace[\"numWaitingVehicles\"]:\n",
    "            for waitingVeh in intersection:\n",
    "                if waitingVeh == 0:\n",
    "                        totalReward += 100\n",
    "                else:\n",
    "                    totalReward -= waitingVeh*100\n",
    "#                 print(\"waitingVeh\", totalReward)\n",
    "                    \n",
    "                        \n",
    "        # avgSpeed reward calc\n",
    "        for intersection in observationSpace[\"numWaitingVehicles\"]:\n",
    "            for speed in intersection:\n",
    "                \n",
    "                #reward if speed is\n",
    "                if (speed-5) >= self.maxSpeed:\n",
    "                    totalReward += 50\n",
    "                else:\n",
    "                    totalReward -= ((self.maxSpeed - speed)/self.maxSpeed)*50\n",
    "#                 print(\"speed\", totalReward)\n",
    "        \n",
    "            \n",
    "        return totalReward/len(self.intersectionNames)\n",
    "            \n",
    "    def reset(self):\n",
    "#         self.engine.reset()\n",
    "        self.currStep = 0\n",
    "        obs = self.get_observation()\n",
    "#         print(\"obs\", obs)\n",
    "        self.isDone = False\n",
    "        return obs\n",
    "    \n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5901a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cityflowEnv = DummyVecEnv([lambda: cityFlowGym(\"sample_data/jacob_config.json\", 3600)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa72c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityflowEnv = cityFlowGym(\"sample_data/jacob_config.json\", 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09352f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24d45e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MultiInputPolicy\", cityflowEnv, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240df6d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 13   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 152  |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.6e+03      |\n",
      "|    ep_rew_mean          | -1.8e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9            |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 449          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.469215e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -5.96e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.48e+11     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -3.78e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.03e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.8e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 6             |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 907           |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0311778e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.58e+11      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -4.97e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.99e+11      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.6e+03      |\n",
      "|    ep_rew_mean          | -1.81e+08    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 1490         |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.386514e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.55e+11     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -1.46e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.09e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 4             |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 2285          |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0035153e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.51e+11      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -2.24e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.97e+11      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 3.6e+03        |\n",
      "|    ep_rew_mean          | -1.81e+08      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 3              |\n",
      "|    iterations           | 6              |\n",
      "|    time_elapsed         | 3269           |\n",
      "|    total_timesteps      | 12288          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.12195266e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.84          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 3.45e+11       |\n",
      "|    n_updates            | 50             |\n",
      "|    policy_gradient_loss | -5.24e-06      |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 7.07e+11       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.6e+03      |\n",
      "|    ep_rew_mean          | -1.81e+08    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 4427         |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.823378e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.58e+11     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -2.49e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.99e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2             |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 5770          |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0381336e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.55e+11      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -4.9e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.09e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2             |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 7614          |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8149453e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.45e+11      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -4.55e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7e+11         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2             |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 9828          |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2965756e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.51e+11      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -6.16e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.97e+11      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.6e+03      |\n",
      "|    ep_rew_mean          | -1.81e+08    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1            |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 11801        |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.070454e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.46e+11     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -5.81e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.08e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 14210         |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8676004e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.44e+11      |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -1.39e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.98e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 16723         |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6479171e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.52e+11      |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -8.59e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.06e+11      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.6e+03      |\n",
      "|    ep_rew_mean          | -1.81e+08    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1            |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 19380        |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.783498e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.47e+11     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -2.08e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.98e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 23034         |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0078733e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.53e+11      |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -1.38e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.07e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 26811         |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4086482e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.49e+11      |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -2.46e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.01e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 29595         |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2386895e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.46e+11      |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | 3.78e-07      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.99e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 32314         |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6056583e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.58e+11      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -1.06e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.07e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 35191         |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4674151e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.56e+11      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -6.19e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.01e+11      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 38200         |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3085387e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.52e+11      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -2.71e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.05e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 41363         |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2604869e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.51e+11      |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -4.66e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.99e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 44669         |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.0486567e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.5e+11       |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -3.84e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.08e+11      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.6e+03      |\n",
      "|    ep_rew_mean          | -1.81e+08    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 48136        |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.914503e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.39e+11     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -5.41e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.97e+11     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 3.6e+03        |\n",
      "|    ep_rew_mean          | -1.81e+08      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 0              |\n",
      "|    iterations           | 24             |\n",
      "|    time_elapsed         | 51762          |\n",
      "|    total_timesteps      | 49152          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.12486305e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.84          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 3.5e+11        |\n",
      "|    n_updates            | 230            |\n",
      "|    policy_gradient_loss | -6.4e-06       |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 7.02e+11       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 55571         |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6735083e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.53e+11      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -3.21e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.08e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 59523         |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9924635e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.52e+11      |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -2.11e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7e+11         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 63687         |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2584572e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.55e+11      |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -1.23e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.08e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 69426         |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6190537e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.49e+11      |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -1.29e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.98e+11      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 74625         |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9744267e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.55e+11      |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -2.35e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.05e+11      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.6e+03      |\n",
      "|    ep_rew_mean          | -1.81e+08    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 79283        |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.071059e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.53e+11     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -2.95e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.99e+11     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.6e+03      |\n",
      "|    ep_rew_mean          | -1.81e+08    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 84736        |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.307367e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.52e+11     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -2.7e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.97e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 91074         |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6248745e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.59e+11      |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -1.52e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.08e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 96497         |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5861588e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.52e+11      |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -8.26e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.96e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | -1.81e+08     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 102695        |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.9663005e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.49e+11      |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -4.02e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.08e+11      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/witeo/Documents/Computer Science/FYP/Reinforcement Learning Tutorial/rlvenv_wsl/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m: PPOSelf,\n\u001b[1;32m    305\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PPOSelf:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/witeo/Documents/Computer Science/FYP/Reinforcement Learning Tutorial/rlvenv_wsl/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:262\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 262\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/witeo/Documents/Computer Science/FYP/Reinforcement Learning Tutorial/rlvenv_wsl/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:181\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    179\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 181\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/witeo/Documents/Computer Science/FYP/Reinforcement Learning Tutorial/rlvenv_wsl/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/witeo/Documents/Computer Science/FYP/Reinforcement Learning Tutorial/rlvenv_wsl/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/mnt/d/witeo/Documents/Computer Science/FYP/Reinforcement Learning Tutorial/rlvenv_wsl/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn[11], line 213\u001b[0m, in \u001b[0;36mcityFlowGym.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    210\u001b[0m             minTimer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m second \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(minTimer):\n\u001b[0;32m--> 213\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m#         obs = np.concatenate((self.engine.get_lane_vehicle_count().reshape(-1, 1), self.engine.get_lane_speed().reshape(-1, 1)), axis=1) / 100\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m#         reward = self.engine.get_reward()\u001b[39;00m\n\u001b[1;32m    217\u001b[0m         obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_observation()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e546e0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/witeo/Documents/Computer Science/FYP/Reinforcement Learning Tutorial/rlvenv_wsl/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"models/PPO\"\n",
    "model.save(f\"{models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36b0ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "for episode in range(1, episodes+1):\n",
    "    state = cityflowEnv.reset()\n",
    "#     cityflowEnv.engine.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    step = 0\n",
    "    \n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = cityflowEnv.action_space.sample()\n",
    "        n_state, reward, done, info = cityflowEnv.step(action)\n",
    "        print(\"action\", action)\n",
    "        print(\"n_state\", n_state)\n",
    "        print(\"reward\", reward)\n",
    "        print(\"done\", done)\n",
    "        print(\"info\", info)\n",
    "        print(\"stepCount\", cityflowEnv.currStep)\n",
    "        score += reward\n",
    "#         break\n",
    "    score /= step\n",
    "        \n",
    "    print('Episode:{} Score:{} with {} steps'.format(episode, score, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b048630",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = cityflowEnv.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3663aaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 53]\n"
     ]
    }
   ],
   "source": [
    "action = cityflowEnv.action_space.sample()\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d59d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_state {'numVehicles': array([[120,  16, 120,  16, 120,  16, 120,  16]]), 'numWaitingVehicles': array([[ 98,   0, 100,   0,  99,   0, 100,   0]]), 'avgSpeed': array([[ 2.33717185, 12.918325  ,  2.11277213, 12.918325  ,  2.18425764,\n",
      "        12.918325  ,  2.04359524, 12.918325  ]])}\n",
      "reward -39844.0\n",
      "done False\n",
      "info {}\n",
      "stepCount 26\n"
     ]
    }
   ],
   "source": [
    "n_state, reward, done, info = cityflowEnv.step(action)\n",
    "\n",
    "print(\"n_state\", n_state)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)\n",
    "print(\"info\", info)\n",
    "print(\"stepCount\", cityflowEnv.currStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9caf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"n_state\", n_state)\n",
    "# print(\"reward\", reward)\n",
    "# print(\"done\", done)\n",
    "# print(\"info\", info)\n",
    "# print(\"stepCount\", cityflowEnv.currStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bcc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c1515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(cityflowEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ea17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actionSpaceArray = []\n",
    "\n",
    "# lightphaseSpace = spaces.Discrete(8)\n",
    "# lightphaseDurationSpace = spaces.Box(low = 0, high=60, shape=(1,), dtype=np.float32)\n",
    "# # intersection1 = gym.spaces.Tuple((lightphaseSpace.n, lightphaseDurationSpace[0]))\n",
    "# actionSpaceArray.append([lightphaseSpace.n, lightphaseDurationSpace.high[0]])\n",
    "# lightphaseSpace = spaces.Discrete(3)\n",
    "# lightphaseDurationSpace = spaces.Box(low = 10, high=20, shape=(1,), dtype=np.int32)\n",
    "# # intersection2 = gym.spaces.Tuple((lightphaseSpace.n, lightphaseDurationSpace[0]))\n",
    "# actionSpaceArray.append([lightphaseSpace.n, lightphaseDurationSpace.high[0]])\n",
    "\n",
    "# action_space = spaces.MultiDiscrete(actionSpaceArray)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for intersection in self.intersections:\n",
    "# lightphaseSpace = spaces.Discrete(8)\n",
    "# lightphaseDurationSpace = spaces.Box(low = 10, high=60, shape=(1,), dtype=np.int32)\n",
    "# actionSpaceArray.append([lightphaseSpace, lightphaseDurationSpace]) # tuple for each intersection (lightphase next, duration)\n",
    "# lightphaseSpace = spaces.Discrete(3)\n",
    "# lightphaseDurationSpace = spaces.Box(low = 10, high=20, shape=(1,), dtype=np.int32)\n",
    "# actionSpaceArray.append([lightphaseSpace, lightphaseDurationSpace]) # tuple for each intersection (lightphase next, duration)\n",
    "\n",
    "# actionSpaceArray.append([i for i in range(8)])\n",
    "# actionSpaceArray.append([i for i in range(10,61)])\n",
    "# actionSpaceArray.append([i for i in range(3)])\n",
    "# actionSpaceArray.append([i for i in range(10,21)])\n",
    "# action_space = spaces.MultiDiscrete(actionSpaceArray) # multidiscrete for all intersections\n",
    "\n",
    "# lightphaseSpace = spaces.Discrete(8)\n",
    "# lightphaseDurationSpace = spaces.Box(low = 10, high=60, shape=(1,), dtype=np.int32)\n",
    "# actionSpaceArray.append(gym.spaces.MultiDiscrete([lightphaseSpace, lightphaseDurationSpace]))\n",
    "# lightphaseSpace = spaces.Discrete(3)\n",
    "# lightphaseDurationSpace = spaces.Box(low = 10, high=20, shape=(1,), dtype=np.int32)\n",
    "# actionSpaceArray.append(gym.spaces.MultiDiscrete([lightphaseSpace, lightphaseDurationSpace]))\n",
    "\n",
    "# action_space = spaces.MultiDiscrete(np.array(actionSpaceArray))\n",
    "\n",
    "# action_space = spaces.MultiDiscrete(actionSpaceArray) # multidiscrete for all intersections\n",
    "\n",
    "\n",
    "action_space = spaces.Box(\n",
    "            np.array([0, 0, 0, 0]).astype(np.int32),\n",
    "            np.array([8, 60, 3, 10]).astype(np.int32),\n",
    "            dtype=np.int32\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.ndarray.tolist(action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974f2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a90c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"cityflow-v0\",\n",
    "    configPath=\"sample_data/jacob_config.json\"\n",
    "    ,episodeSteps=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.getRoadnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcefe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check action space\n",
    "print(type(env.action_space))\n",
    "print(type(env.observation_space))\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de017de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e375652",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.actionSpaceArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11510542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572ca11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df6990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.intersections[\"intersection_1_1\"][1])\n",
    "print(len(env.intersections[\"intersection_1_1\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.intersections[\"intersection_1_1\"][1][0])\n",
    "print(len(env.intersections[\"intersection_1_1\"][1][0]))\n",
    "print()\n",
    "print(env.intersections[\"intersection_1_1\"][1][1])\n",
    "print(len(env.intersections[\"intersection_1_1\"][1][1]))\n",
    "print()\n",
    "print(env.intersections[\"intersection_1_1\"][1][2])\n",
    "print(len(env.intersections[\"intersection_1_1\"][1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.intersections[\"intersection_1_1\"][2][0])\n",
    "print(len(env.intersections[\"intersection_1_1\"][2][0]))\n",
    "print()\n",
    "print(env.intersections[\"intersection_1_1\"][2][1])\n",
    "print(len(env.intersections[\"intersection_1_1\"][2][1]))\n",
    "print()\n",
    "print(env.intersections[\"intersection_1_1\"][2][2])\n",
    "print(len(env.intersections[\"intersection_1_1\"][2][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.intersections[\"intersection_1_1\"][3])\n",
    "print(len(env.intersections[\"intersection_1_1\"][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb344839",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.flowDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d492f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.flowDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(env.intersections[\"intersection_1_1\"][1])):\n",
    "    print(\"New i \" + str(i))\n",
    "    for j in range(len(env.intersections[\"intersection_1_1\"][1][i])):\n",
    "        print(\"Start Road \" + env.intersections[\"intersection_1_1\"][1][i][j] + \n",
    "              \" End Road \" + env.intersections[\"intersection_1_1\"][2][i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # disable print temporarily\n",
    "# # iterate environment a lttle bit to test env\n",
    "# actionInterval = 10\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     if i % actionInterval == 0:\n",
    "#         testAction = []\n",
    "#         for i in range(0, 16):\n",
    "#             n = random.randint(0, 8)\n",
    "#             testAction.append(n)\n",
    "#     observation, reward, done, debug = env.step(action=testAction)\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "# observation, reward, done, debug = env.step(action=testAction)\n",
    "# print(observation)\n",
    "# print(reward)\n",
    "\n",
    "# observation = env.reset()\n",
    "# print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    step = 0\n",
    "    \n",
    "    while not done:\n",
    "#         env.render()\n",
    "        step += 1\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        \n",
    "        print(reward)\n",
    "        avg_reward = 0\n",
    "        for intersection in reward:\n",
    "            avg_reward += intersection[1]\n",
    "        avg_reward /= len(reward)\n",
    "        score += avg_reward\n",
    "        \n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5a606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd5494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bda851",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityflowEnv = Cityflow(\"sample_data/jacob_config.json\", 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MultiInputPolicy\", cityflowEnv, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(env.observation_space.spaces))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6975d7",
   "metadata": {},
   "source": [
    "## Simulation of Junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cityflow\n",
    "eng_conf = cityflow.Engine(\"sample_data/jacob_config.json\", thread_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db443879",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    eng_conf.next_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29086835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlvenv_wsl",
   "language": "python",
   "name": "rlvenv_wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
